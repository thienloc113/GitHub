install.packages("ggplot2")
aes()
check()
install.packages("devtools")
install.packages("KernSmooth")
library()
library(KernSmooth)
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if(x > 5) {
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
h <- function(x, y = NULL, d = 3L) {
z <- cbind(x, d)
if(!is.null(y))
z <- z + y
else
z <- z + f
g <- x + y / z
if(d == 3L)
return(g)
g <- g + 10
g
}
dataset_url <- "http://s3.amazonaws.com/practice_assignment/diet_data.zip"
download.file(dataset_url, "diet_data.zip")
unzip("diet_data.zip", exdir = "diet_data")
list.files("diet_data")
andy <- read.csv("diet_data/Andy.csv")
head(andy)
length(andy$Day)
andy[1, "Weight"]
andy[30, "Weight"]
andy[which(andy$Day == 30), "Weight"]
andy[which(andy[,"Day"] == 30), "Weight"]
andy_start <- andy[1, "Weight"]
andy_end <- andy[30, "Weight"]
andy_loss <- andy_start - andy_end
andy_loss
files <- list.files("diet_data")
files
{r error=TRUE}
head(read.csv(files[3]))
head(read.csv(files[3]))
files_full <- list.files("diet_data", full.names=TRUE)
files_full
head(read.csv(files_full[3]))
andy_david <- rbind(andy, read.csv(files_full[2]))
head(andy_david)
tail(andy_david)
day_25 <- andy_david[which(andy_david$Day == 25), ]
day_25
for (i in 1:5) {print(i)}
{r error=TRUE}
for (i in 1:5) {
dat <- rbind(dat, read.csv(files_full[i]))
}
dat <- data.frame()
for (i in 1:5) {
dat <- rbind(dat, read.csv(files_full[i]))
}
str(dat)
for (i in 1:5) {
dat2 <- data.frame()
dat2 <- rbind(dat2, read.csv(files_full[i]))
}
str(dat2)
head(dat2)
median(dat$Weight)
median(dat$Weight, na.rm=TRUE)
dat_30 <- dat[which(dat[, "Day"] == 30),]
dat_30
median(dat_30$Weight)
`weightmedian <- function(directory, day)  {
# content of the function
}`
weightmedian <- function(directory, day)  {
# content of the function
}
{r tidy=TRUE, tidy.opts=list(width.cutoff=60)}
weightmedian <- function(directory, day)  {
files_list <- list.files(directory, full.names=TRUE)   #creates a list of files
dat <- data.frame()                             #creates an empty data frame
for (i in 1:5) {
#loops through the files, rbinding them together
dat <- rbind(dat, read.csv(files_list[i]))
}
dat_subset <- dat[which(dat[, "Day"] == day),]  #subsets the rows that match the 'day' argument
median(dat_subset[, "Weight"], na.rm=TRUE)      #identifies the median weight
#while stripping out the NAs
}
summary(files_full)
tmp <- vector(mode = "list", length = length(files_full))
summary(tmp)
```
Now we need to read in those csv files and drop them into `tmp`.
```{r}
for (i in seq_along(files_full)) {
tmp[[i]] <- read.csv(files_full[[i]])
}
str(tmp)
str(lapply(files_full, read.csv))
str(tmp[[1]])
head(tmp[[1]][,"Day"])
output <- do.call(rbind, tmp)
str(output)
# install.packages("data.table")
library("data.table")
pollutantmean <- function(directory, pollutant, id = 1:332) {
# Format number with fixed width and then append .csv to number
fileNames <- paste0(directory, '/', formatC(id, width=3, flag="0"), ".csv" )
# Reading in all files and making a large data.table
lst <- lapply(fileNames, data.table::fread)
dt <- rbindlist(lst)
if (c(pollutant) %in% names(dt)){
return(dt[, lapply(.SD, mean, na.rm = TRUE), .SDcols = pollutant][[1]])
}
}
# Example usage
pollutantmean(directory = '~/Desktop/specdata', pollutant = 'sulfate', id = 20)
complete <- function(directory,  id = 1:332) {
# Format number with fixed width and then append .csv to number
fileNames <- paste0(directory, '/', formatC(id, width=3, flag="0"), ".csv" )
# Reading in all files and making a large data.table
lst <- lapply(fileNames, data.table::fread)
dt <- rbindlist(lst)
return(dt[complete.cases(dt), .(nobs = .N), by = ID])
}
#Example usage
complete(directory = '~/Desktop/specdata', id = 20:30)
corr <- function(directory, threshold = 0) {
# Reading in all files and making a large data.table
lst <- lapply(file.path(directory, list.files(path = directory, pattern="*.csv")), data.table::fread)
dt <- rbindlist(lst)
# Only keep completely observed cases
dt <- dt[complete.cases(dt),]
# Apply threshold
dt <- dt[, .(nobs = .N, corr = cor(x = sulfate, y = nitrate)), by = ID][nobs > threshold]
return(dt[, corr])
}
# Example Usage
corr(directory = '~/Desktop/specdata', threshold = 150)
pollutantmean("specdata", "sulfate", 1:10)
getwd
getwd()
View(andy_david)
invMatrix <- NULL
x<-matrix(1:9,3,3)
set<-function(y){}
>fread
?fread
??fread
?sapply
?require
?read.table
getwd()
source('~/GitHub/Getting-and-Cleaning-data/run_analysis.R')
options(prompt = "ISIK>")
a
a<-5
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
?lline
??llines
?llines
df <- data.frame(x = 1, y = 3:1, family = c("sans", "serif", "mono"))
ggplot(df, aes(x, y)) +
geom_text(aes(label = family, family = family))
library(ggplot2)
df <- data.frame(x = 1, y = 3:1, family = c("sans", "serif", "mono"))
ggplot(df, aes(x, y)) +
geom_text(aes(label = family, family = family))
df <- data.frame(x = 1, y = 3:1, family = c("sans", "serif", "mono"))
ggplot(df, aes(x, y)) +
geom_text(aes(label = c(1:3), family = family))
2
df <- data.frame(x = 1, y = 3:1, family = c("sans", "serif", "mono"))
ggplot(df, aes(x, y)) +
geom_text(aes(label = c(1:3), family = "sans"))
economics
?xrmg
?xrng
range(c(1:3))
ggplot(economics) +
geom_rect(
aes(xmin = start, xmax = end, fill = party),
ymin = -Inf, ymax = Inf, alpha = 0.2,
data = presidential
) +
geom_vline(
aes(xintercept = as.numeric(start)),
data = presidential,
colour = "grey50", alpha = 0.5
)
Views(economics)
View(economics)
economics$start
?start
ggplot(economics) +
+     geom_rect(
+         aes(xmin = start, xmax = end, fill = party),
+         ymin = -Inf, ymax = Inf, alpha = 0.2,
+         data = presidential
+     ) +
+     geom_vline(
+         aes(xintercept = as.numeric(start)),
+         data = presidential,
+         colour = "grey50", alpha = 0.5
+     )
ggplot(economics) +
geom_rect(
aes(xmin = start, xmax = end, fill = party),
ymin = -Inf, ymax = Inf, alpha = 0.2,
data = presidential
) +
geom_vline(
aes(xintercept = as.numeric(start)),
data = presidential,
colour = "grey50", alpha = 0.5
)
shiny::runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
runApp('C:/Users/ASUS/Desktop/MOOC/Data Analyst Hopskin/DDP/Testing')
install.packages("leaflet")
library(leaflet)
my_map <- leaflet()
my_map <- addTiles(my_map)
my_map
a <- leaflet()
a
my_map <- my_map %>%
addMarkers(lat = 39, lng = -76, popup = "Office")
my_map
?gglot2
library(ggplot2)
?ggplot2
library(shiny)
?checkboxGroupInput
c("Cylinders" = "cyl",
"Transmission" = "am",
"Gears" = "gear")
library(tidyverse)
data <- read.csv("C:\Users\ASUS\Documents\GitHub\DDP\Week 4\W4_Assignment\worldwide internet users - users.csv")
data <- read.csv("C:/Users/ASUS/Documents/GitHub/DDP/Week 4/W4_Assignment/worldwide internet users - users.csv")
data
distinct(data, Region)
L_region <- distinct(data, Region)
?distinct
L_region <- distinct(data, Region)[, 1]
?checkboxGroupInput
shiny::runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
library(markdown)
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
data$Population <- as.numeric(gsub(",","",data$Population))
data$Internet.users <- as.numeric(gsub(",","",data$Internet.users))
runApp('GitHub/DDP/Week 4/W4_Assignment')
runApp('GitHub/DDP/Week 4/W4_Assignment')
